{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3ee989",
   "metadata": {},
   "source": [
    "# Este cuaderno busca la mejor red neuronal para predecir la salida de metas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7062fd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juanm\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio actual: C:\\Users\\juanm\\Dropbox\\JP_files\\UR\\Vías clinicas diabetes\n"
     ]
    }
   ],
   "source": [
    "## Liberías necesarias \n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import shap\n",
    "import socket \n",
    "import joblib\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "## Configuración de rutas\n",
    "if socket.gethostname()=='SRVCBECO01':\n",
    "    os.chdir('D:\\shared_data\\Dropbox\\Vías clinicas diabetes')\n",
    "elif socket.gethostname() == 'HPJP2': \n",
    "    os.chdir(r'C:\\Users\\juanm\\Dropbox\\JP_files\\UR\\Vías clinicas diabetes')\n",
    "elif socket.gethostname() == 'CNF106054': \n",
    "    os.chdir(r'C:\\Users\\paul.rodriguez\\Dropbox\\Salud Colombia\\Diabetes Sanitas\\Vías clinicas diabetes')\n",
    "elif socket.gethostname() == 'CNF77701': \n",
    "    os.chdir(r'C:\\Users\\juanpablo.martinez\\Dropbox\\Vías clinicas diabetes')\n",
    "\n",
    "    \n",
    "raw_path = 'Datos org\\\\csv_rutas\\\\'\n",
    "created_path = \"Datos creados\\\\\"\n",
    "temp_path = 'temporales\\\\'\n",
    "dbs_path = \"Datos creados\\\\ml_databases\\\\4_cat\\\\\"\n",
    "\n",
    "\n",
    "print('Directorio actual: '+ os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc518db",
   "metadata": {},
   "source": [
    "### Funciones "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b2f5f6",
   "metadata": {},
   "source": [
    "#### Preparación de las bases de entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2017544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare(df_path, vars_to_drop):\n",
    "    ## Cargando la base de datos\n",
    "    data_mat = pd.read_csv(df_path)\n",
    "    \n",
    "    ## Eliminando variables innecesarias\n",
    "    data_mat.drop(vars_to_drop, axis = 1, inplace = True)\n",
    "    \n",
    "    return(data_mat.reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1237cbba",
   "metadata": {},
   "source": [
    "#### Identifcando variables categóricas y numéricas en la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f1aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_num_identify(ml_data, rename_df):\n",
    "    categorical_vars_master = ['femenino', 'ERC_high', 'ERC1', 'ERC2', 'adhiere_guia', 'no_adhiere', 'adhiere_colesterol', 'analgesicos',\n",
    "                                   'antiacidos', 'antihipertensivos', 'hipoglicemiantes', 'hipolipemiantes', 'nutrition_tag_max', \n",
    "                                   'exercise_tag_max', 'alcohol_tag_max', 'tobaco_tag_max', 'creatinina_missing']\n",
    "\n",
    "    categorical_vars = list(set(ml_data.columns).intersection(categorical_vars_master))\n",
    "\n",
    "    numerical_vars = set(ml_data.columns).difference(set(categorical_vars+['KeyAnonimo', 'year', 'base_label', 'comorbilidades', 'fuera_metas', 'tgt_label']))\n",
    "    numerical_vars = list(numerical_vars)\n",
    "\n",
    "    types_df = pd.DataFrame({'old_name' : categorical_vars+numerical_vars,\n",
    "                             'type' : ['categorical']*len(categorical_vars)+['numerical']*len(numerical_vars)})\n",
    "\n",
    "    types_df = types_df.merge(rename_df, on = 'old_name', how = 'inner')\n",
    "\n",
    "    new_cat_vars = list(types_df.loc[types_df['type'] == 'categorical', 'new_name'].values)\n",
    "    new_num_vars = list(types_df.loc[types_df['type'] == 'numerical', 'new_name'].values)\n",
    "    \n",
    "    return new_cat_vars, new_num_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f325dee",
   "metadata": {},
   "source": [
    "#### Partición entre variables exógenas y endógenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7c3dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def endog_exog_select(data_mat, dep_var):\n",
    "    X = data_mat.drop(dep_var, axis = 1)\n",
    "    Y = data_mat.loc[:, dep_var]\n",
    "    return(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f304aa",
   "metadata": {},
   "source": [
    "#### Wrapper de la preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c65bc2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare_wrapper():\n",
    "    ## Construyendo los paths de entrenamiento y validación\n",
    "    train_path = dbs_path+\"train\\\\train_db_{}_years_base_{}.csv\".format(tgt_year, base_label)\n",
    "    vali_path = dbs_path+\"vali\\\\vali_db_{}_years_base_{}.csv\".format(tgt_year, base_label)\n",
    "\n",
    "    ## Listado de variables a eliminar \n",
    "    vars_to_drop = ['KeyAnonimo', 'year', 'base_label', 'tgt_label']\n",
    "    if dep_var == \"comorbilidades\":\n",
    "        vars_to_drop = vars_to_drop+['fuera_metas']\n",
    "    elif dep_var == \"fuera_metas\":\n",
    "        vars_to_drop = vars_to_drop+['comorbilidades']\n",
    "\n",
    "    ## Diccionario para renombrar variables\n",
    "    rename_dict ={'femenino' : 'Female', 'edad' : 'Age (Years)', 'peso': 'Weight (Kg)', 'talla' : 'Height (m)', \n",
    "                  'imc' : \"BMI (kg/m*m)\", 'Colesterol_LDL' : 'LDL Chol. (mg/dL)', 'TFG' : \"eGFR (mg/g)\", \n",
    "                  \"ta_diastolica\" : \"Diast. B.P. (mmHg)\", 'ta_sistolica' : 'Sist. B.P. (mmHg)', 'adhiere_guia' : \"Hba1c guide Adh.\", \n",
    "                  \"no_adhiere\" : \"Pharma. Adh.\", \"analgesicos\" : \"Analgesics\", \"antiacidos\" : \"Antacdis\",\n",
    "                  \"antihipertensivos\" : \"Antihypertensive\", \"hipoglicemiantes\" : \"Hypoglecimic agents\", \n",
    "                  \"hipolipemiantes\" : \"Lipid-lowering agents\", \"nutrition_tag_max\" : \"Nutrition recomm.\", \n",
    "                  \"exercise_tag_max\" : \"Physical act. recomm.\", \"alcohol_tag_max\" : \"Alcohol recomm.\",\n",
    "                  \"tobaco_tag_max\" : \"Tobacco recomm.\", \"creatinina\" : \"Creatinine (mg/dL)\", \n",
    "                  \"adhiere_colesterol\" : \"Chol. Adh.\"} \n",
    "\n",
    "    #'imc': r\"BMI ($\\displaystyle\\frac{kg}{m^2}$)\"}\n",
    "\n",
    "    rename_df = pd.DataFrame(rename_dict, index = rename_dict.keys())\n",
    "    rename_df = pd.DataFrame({'old_name' : rename_df.index, 'new_name' : np.diag(rename_df)})\n",
    "    \n",
    "    ## Preparando los datos\n",
    "    train = data_prepare(df_path = train_path, vars_to_drop = vars_to_drop)\n",
    "    vali = data_prepare(df_path = vali_path, vars_to_drop = vars_to_drop)\n",
    "\n",
    "    ## Identificando variables categóricas y numéricas presentes\n",
    "    cat_vars, num_vars = cat_num_identify(ml_data = train, rename_df = rename_df)\n",
    "\n",
    "    ## Renombrando las variables para el gráfico\n",
    "    train.rename(rename_dict, axis = 1, inplace = True)\n",
    "    vali.rename(rename_dict, axis = 1, inplace = True)\n",
    "\n",
    "    ## Segmentación entre variables endógenas y exógenas \n",
    "    X_train, Y_train = endog_exog_select(data_mat = train,\n",
    "                                             dep_var = 'fuera_metas')\n",
    "                                             #dep_var = 'comorbilidades')\n",
    "    X_vali, Y_vali = endog_exog_select(data_mat = vali, \n",
    "                                       dep_var = 'fuera_metas')\n",
    "                                    #   dep_var = 'comorbilidades')\n",
    "        \n",
    "    return X_train, Y_train, X_vali, Y_vali"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a8e7f8",
   "metadata": {},
   "source": [
    "#### Incialización del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a20e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Función para instanciar el modelo \n",
    "def model_init(reg_param, \n",
    "               input_shape, \n",
    "               output_shape):\n",
    "    model = tf.keras.Sequential([tf.keras.layers.Dense(256,\n",
    "                                             activation=\"relu\", \n",
    "                                             input_shape=[input_shape]),\n",
    "                          tf.keras.layers.Dense(256, \n",
    "                                             activation=\"relu\"),\n",
    "                          tf.keras.layers.Dropout(reg_param),\n",
    "                          tf.keras.layers.Dense(256,\n",
    "                                             activation=\"relu\"),\n",
    "                          tf.keras.layers.Dropout(reg_param),\n",
    "                         tf.keras.layers.Dense(output_shape,\n",
    "                                             activation=\"sigmoid\")\n",
    "                         ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3fad18",
   "metadata": {},
   "source": [
    "#### Compilación y ejecución del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8383e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_exe(reg_param, \n",
    "                    batch_size,\n",
    "                    epochs,\n",
    "                    lr_init, \n",
    "                    weighted):\n",
    "    # Incializando el modelo \n",
    "    model = model_init(reg_param = reg_param, \n",
    "                       input_shape = len(X_train.columns),\n",
    "                       output_shape = len(pd.DataFrame(Y_train).columns))\n",
    "\n",
    "    ## Compilación del modelo \n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=lr_init),\n",
    "                  loss = \"binary_crossentropy\")\n",
    "    \n",
    "    ## Si el modelo es con pesos, computarlos y añadirlos \n",
    "    if weighted:\n",
    "        ## Construcción de los pesos \n",
    "        counts = np.bincount(Y_train)\n",
    "        class_weights = {0: 1.0 / counts[0], 1: 1.0 / counts[1]}\n",
    "        \n",
    "        ## Entrenamiento del modelo \n",
    "        model.fit(X_train,\n",
    "                  Y_train,\n",
    "                  validation_data = (X_vali,\n",
    "                                     Y_vali),\n",
    "                  batch_size = batch_size,\n",
    "                  epochs = epochs,\n",
    "                  verbose = 0, \n",
    "                  class_weight = class_weights\n",
    "                 )\n",
    "\n",
    "    else:\n",
    "        ## Entrenamiento del modelo \n",
    "        model.fit(X_train,\n",
    "                  Y_train,\n",
    "                  validation_data = (X_vali,\n",
    "                                     Y_vali),\n",
    "                  batch_size = batch_size,\n",
    "                  epochs = epochs,\n",
    "                  verbose = 0, \n",
    "                #  class_weight = class_weights\n",
    "                 )\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d764d0e",
   "metadata": {},
   "source": [
    "#### Cómputo del corte óptimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50a661df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_cutoff_finder(predicted_probabilities, delta, endog_vars):\n",
    "    f1_res = []\n",
    "\n",
    "    for pr_limit in np.arange(0, 1, delta):\n",
    "        ## Asignando la etiqueta en función del corte óptimo\n",
    "        predicted_labels = np.where(predicted_probabilities>pr_limit, 1, 0)\n",
    "\n",
    "        ## F1-score\n",
    "        f1_res.append(f1_score(endog_vars, predicted_labels, average = 'weighted'))\n",
    "\n",
    "    f1_compiled = pd.DataFrame({'f1_score' : f1_res})\n",
    "    f1_compiled.index = np.arange(0, 1, delta)\n",
    "\n",
    "    return f1_compiled.idxmax()['f1_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df5bbaf",
   "metadata": {},
   "source": [
    "#### Compilación y ejecución de la evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aedac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluator(predicted_probabilities, samp_type, endog_vars, pr_limit):\n",
    "\n",
    "    # Prediciendo etiquetas \n",
    "    predicted_labels = np.where(predicted_probabilities>pr_limit, 1, 0)\n",
    "\n",
    "    # Computando métricas \n",
    "    accuracy = accuracy_score(endog_vars, predicted_labels)\n",
    "    auc = round(roc_auc_score(y_true = endog_vars, y_score = predicted_probabilities), 4)\n",
    "    recall = round(recall_score(y_true = endog_vars, y_pred = predicted_labels), 4)\n",
    "    prec = round(precision_score(y_true = endog_vars, y_pred = predicted_labels), 4)\n",
    "    f1 = f1_score(endog_vars, predicted_labels, average = 'weighted')\n",
    "    scores = [tgt_year, base_label, dep_var, samp_type,\n",
    "              weighted, epochs, reg_param, batch_size, lr_init,\n",
    "              pr_limit, accuracy, auc, recall, prec, f1]\n",
    "\n",
    "    # Generando la matriz de resultados \n",
    "    final_info = pd.DataFrame()\n",
    "    final_info.loc[0, ['tgt_year', 'base_label', 'dep_var', 'samp_type', \n",
    "                       \"weighted\", \"epochs\", \"reg_param\", \"batch_size\", \"lr\",\n",
    "                       'cutoff', 'accuracy', 'auc', 'recall', 'prec', 'f1']] = scores\n",
    "    \n",
    "    return final_info\n",
    "\n",
    "def all_res_wrapper():\n",
    "    ## Computando métricas con ambos cortes para entrenamiento y validación\n",
    "    train_naive = model_evaluator(predicted_probabilities = train_probs, \n",
    "                    samp_type = 'training',\n",
    "                    endog_vars = Y_train,\n",
    "                    pr_limit = 0.5)\n",
    "\n",
    "    train_optimal = model_evaluator(predicted_probabilities = train_probs, \n",
    "                                    samp_type = 'training',\n",
    "                                    endog_vars = Y_train,\n",
    "                                    pr_limit = cutoff)\n",
    "\n",
    "    vali_naive = model_evaluator(predicted_probabilities = vali_probs, \n",
    "                                    samp_type = 'validation',\n",
    "                                    endog_vars = Y_vali,\n",
    "                                    pr_limit = 0.5)\n",
    "\n",
    "    vali_optimal = model_evaluator(predicted_probabilities = vali_probs, \n",
    "                                    samp_type = 'validation',\n",
    "                                    endog_vars = Y_vali,\n",
    "                                    pr_limit = cutoff)\n",
    "\n",
    "    return pd.concat([train_naive, train_optimal, vali_naive, vali_optimal], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df0f4e1",
   "metadata": {},
   "source": [
    "## Entrenamiento para varios estadios base y horizontes temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c64a012",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "### Parámetros básicos\n",
    "## Horizonte temporal, estadío base y variable a predecir\n",
    "for tgt_year in [1, 2]:\n",
    "    for base_label in [1, 3]:\n",
    "        dep_var = 'fuera_metas'\n",
    "\n",
    "        ### Ejecución de los modelos\n",
    "        ## Lista vacía para llenar en cada iteración \n",
    "        all_results = []\n",
    "\n",
    "        ## Organización de los datos \n",
    "        X_train, Y_train, X_vali, Y_vali = data_prepare_wrapper()\n",
    "\n",
    "        ## Hiperparametros red neuronal    \n",
    "        for weighted in [False, True]:\n",
    "            for epochs in [20, 50]:\n",
    "                for reg_param in [0.2, 0.3, 0.5]:\n",
    "                    for batch_size in [50, 100, 150]:\n",
    "                        for lr_init in [0.0001, 0.0005, 0.001]:\n",
    "\n",
    "                            ## Entrenamiento del modelo \n",
    "                            trained_model = model_train_exe(reg_param = reg_param, \n",
    "                                                            batch_size = batch_size, \n",
    "                                                            epochs = epochs, \n",
    "                                                            lr_init = lr_init, \n",
    "                                                            weighted = weighted)\n",
    "\n",
    "                            ## Predicciones del modelo \n",
    "                            train_probs = trained_model.predict(X_train, verbose = 0)\n",
    "                            vali_probs = trained_model.predict(X_vali, verbose = 0)\n",
    "\n",
    "                            ## Hallando el corte óptimo\n",
    "                            cutoff = optimal_cutoff_finder(predicted_probabilities = vali_probs, delta = 0.001, endog_vars = Y_vali)\n",
    "\n",
    "                            ## Generando los resultados finales en comparación al output de pycaret\n",
    "                            all_results.append(all_res_wrapper())\n",
    "\n",
    "                            pd.concat(all_results, \n",
    "                                      axis = 0,\n",
    "                                      ignore_index = True).to_csv(temp_path+'nn_res_{}_{}_years_base_{}.csv'.format(dep_var, \n",
    "                                                                                                                    tgt_year,\n",
    "                                                                                                                    base_label), \n",
    "                                                                 index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f85291",
   "metadata": {},
   "source": [
    "### Escogiendo el mejor modelo y recreándolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a741b03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt_year              1.0\n",
      "base_label            1.0\n",
      "dep_var       fuera_metas\n",
      "samp_type      validation\n",
      "weighted            False\n",
      "epochs               20.0\n",
      "reg_param             0.3\n",
      "batch_size           50.0\n",
      "lr                 0.0005\n",
      "cutoff                0.5\n",
      "accuracy         0.806867\n",
      "auc                0.5805\n",
      "recall             0.0968\n",
      "prec               0.3429\n",
      "f1               0.759751\n",
      "Name: 21, dtype: object\n",
      "tgt_year              1.0\n",
      "base_label            3.0\n",
      "dep_var       fuera_metas\n",
      "samp_type      validation\n",
      "weighted            False\n",
      "epochs               50.0\n",
      "reg_param             0.3\n",
      "batch_size          100.0\n",
      "lr                 0.0001\n",
      "cutoff                0.5\n",
      "accuracy         0.803642\n",
      "auc                0.5679\n",
      "recall             0.0527\n",
      "prec               0.3472\n",
      "f1               0.740116\n",
      "Name: 79, dtype: object\n",
      "tgt_year              2.0\n",
      "base_label            1.0\n",
      "dep_var       fuera_metas\n",
      "samp_type      validation\n",
      "weighted             True\n",
      "epochs               20.0\n",
      "reg_param             0.5\n",
      "batch_size          150.0\n",
      "lr                  0.001\n",
      "cutoff                0.5\n",
      "accuracy         0.690554\n",
      "auc                0.6197\n",
      "recall             0.3828\n",
      "prec                0.339\n",
      "f1               0.696951\n",
      "Name: 161, dtype: object\n",
      "tgt_year              2.0\n",
      "base_label            3.0\n",
      "dep_var       fuera_metas\n",
      "samp_type      validation\n",
      "weighted            False\n",
      "epochs               20.0\n",
      "reg_param             0.3\n",
      "batch_size           50.0\n",
      "lr                  0.001\n",
      "cutoff                0.5\n",
      "accuracy          0.76383\n",
      "auc                0.5543\n",
      "recall             0.1746\n",
      "prec               0.3333\n",
      "f1               0.733604\n",
      "Name: 23, dtype: object\n"
     ]
    }
   ],
   "source": [
    "### Parámetros básicos\n",
    "## Horizonte temporal, estadío base y variable a predecir\n",
    "for tgt_year in [1, 2]:\n",
    "    for base_label in [1, 3]:\n",
    "        dep_var = 'fuera_metas'\n",
    "\n",
    "        ### Ejecución de los modelos\n",
    "        ## Lista vacía para llenar en cada iteración \n",
    "        all_results = []\n",
    "\n",
    "        ## Organización de los datos \n",
    "        X_train, Y_train, X_vali, Y_vali = data_prepare_wrapper()\n",
    "\n",
    "\n",
    "        all_results = pd.read_csv(temp_path+'nn_res_{}_{}_years_base_{}.csv'.format(dep_var, \n",
    "                                                                                    tgt_year,\n",
    "                                                                                    base_label))\n",
    "        \n",
    "        # Keep metrics evaluated at naive cutoff\n",
    "        all_results = all_results[all_results['cutoff'] == 0.5].reset_index(drop = True)\n",
    "\n",
    "        best_model = all_results.loc[(all_results['samp_type'] == \"validation\"), 'f1'].idxmax()\n",
    "        best_model = all_results.loc[best_model, :]\n",
    "\n",
    "        ## Re-entrenamiento del modelo \n",
    "        trained_model = model_train_exe(reg_param = best_model[\"reg_param\"], \n",
    "                                        batch_size = best_model[\"batch_size\"].astype(int), \n",
    "                                        epochs = best_model[\"epochs\"].astype(int), \n",
    "                                        lr_init = best_model[\"lr\"], \n",
    "                                        weighted = best_model['weighted'])\n",
    "\n",
    "        ## Exportando el modelo \n",
    "        saving_path = created_path+\"ml_models\\\\final\\\\{}_{}_years_base_{}\".format(dep_var, tgt_year, base_label)\n",
    "        trained_model.save(saving_path+'.keras')\n",
    "        #joblib.dump(trained_model, saving_path+'.pkl')\n",
    "        \n",
    "        ## Exportando las probabilidades y etiquetas del modelo\n",
    "        pred_res = pd.concat([pd.DataFrame({\"sample\" : np.repeat(\"training\", len(X_train)), \n",
    "                                              \"probabilities\" : trained_model.predict(X_train, verbose = 0)[:, 0], \n",
    "                                              \"labels\" : Y_train}),\n",
    "                                pd.DataFrame({\"sample\" : np.repeat(\"validation\", len(X_vali)), \n",
    "                                              \"probabilities\" : trained_model.predict(X_vali, verbose = 0)[:, 0], \n",
    "                                              \"labels\" : Y_vali})], \n",
    "                                axis = 0)\n",
    "        pred_res['dep_var'] = dep_var\n",
    "        pred_res['base_label'] = base_label\n",
    "        pred_res['tgt_year'] = tgt_year\n",
    "        pred_res['model_type'] = \"NN\"\n",
    "\n",
    "        pred_res.to_csv(temp_path+\"ryr_results\\\\baseline_replicate\\\\predicted_probabilities\\\\{}_{}_years_base_{}.csv\".format(dep_var, tgt_year, base_label), \n",
    "                        sep = ';', \n",
    "                        index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
